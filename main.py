"""Main loop for robotic arm control using LLM and vision."""

"""Main loop for robotic arm control using LLM and vision."""

import cv2
from vision.object_detector import ObjectDetector
from arm_control.kinematics import calculate_ik
from arm_control.arduino_controller import ArduinoController
from llm.interface import LLMController

def main():
    # âœ… Initialize camera, detector, LLM, and robot arm
    camera = cv2.VideoCapture(0)
    detector = ObjectDetector(debug=True)
    llm = LLMController()
    arm = ArduinoController(port="COM5")  # Update COM3 to your actual port

    print("ğŸ¤– LLM Robotic Arm Controller Started")
    arm.home_position()

    try:
        while True:
            # ğŸ¥ Capture frame
            ret, frame = camera.read()
            if not ret:
                print("âŒ Camera frame capture failed.")
                break

            # ğŸ” Detect objects using OpenCV + camera calibration
            detected_objects = detector.detect_objects(frame)
            for obj in detected_objects:
                print(f"ğŸŸ¢ {obj['label']} at {obj['center_mm']} mm")

            # ğŸ—£ï¸ Take user input (text command)
            user_command = input("ğŸ’¬ Enter command (or 'exit'): ")
            if user_command.lower() == 'exit':
                break

            # ğŸ“¦ Prepare LLM input format
            objects_seen = [
                {
                    "label": obj["label"],
                    "position": {
                        "x": round(obj["center_mm"][0], 2),
                        "y": round(obj["center_mm"][1], 2),
                        "z": round(obj["center_mm"][2], 2)
                    }
                }
                for obj in detected_objects
            ]

            # âœ‰ï¸ Prompt sent to LLM (GPT-4 Vision if image is supported)
            prompt = f"""
You are controlling a 5-DOF robotic arm in a real workspace.

Here are the objects detected:
{objects_seen}

User command:
"{user_command}"

Choose the correct object and return a JSON command:

- For movement:
  {{ "command": "MOVE", "target": [x, y, z] }}
- For gripper control:
  {{ "command": "GRIP", "gripper": "open" or "close" }}

Use ONLY the object positions provided above. Do not guess.
Only return one JSON object. Do not include explanations.
"""

            print("ğŸ“¤ Sending prompt to LLM...")
            response = llm.ask(prompt, image=frame)  # Send with image if vision model is used
            print("ğŸ¤– LLM Response:", response)

            # âœ… Execute the LLM-decided action
            if response.get("command") == "MOVE":
                x, y, z = response["target"]
                print(f"ğŸ¯ Moving to: {x}, {y}, {z}")
                joint_angles = calculate_ik(x, y, z)

                from utils.safety import check_joint_limits

                if joint_angles and check_joint_limits(joint_angles):
                    arm.move_to(joint_angles)
                    print("âœ… Arm moved successfully.")
                else:
                    print("âŒ Target unreachable or outside joint limits or exceeded safety limits.")

            elif response.get("command") == "GRIP":
                action = response["gripper"]
                print(f"âœŠ Gripper command: {action}")
                arm.control_gripper(action)

            else:
                print("âš ï¸ Invalid or unrecognized response from LLM.")

    except KeyboardInterrupt:
        print("\nğŸ›‘ Interrupted by user. Exiting...")

    finally:
        camera.release()
        arm.emergency_stop()
        cv2.destroyAllWindows()
        print("ğŸ‘‹ Shutdown complete.")

if __name__ == "__main__":
    main()
